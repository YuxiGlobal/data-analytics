{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dw_utils3 module (v. 20181113)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import math\n",
    "from matplotlib import pyplot as plt\n",
    "from importlib import reload \n",
    "import sys \n",
    "sys.path.append( \"../\")\n",
    "import dw_utils3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating submitter function and submitting test question to verify connection.\n",
      "Q00 = 'test answer'\n",
      "hostname '52.91.20.10' doesn't match 'data-workshops.yuxiglobal.com'\n"
     ]
    }
   ],
   "source": [
    "ans_submit = dw_utils3.create_submitter(host='52.91.20.10', port=443, \n",
    "                       user=\"your.name@yuxiglobal.com\", # put your full yuxi email address here, including @yuxiglobal.com\n",
    "                       ws_key=\"dw3\", #this is the workshop key, don't change it \n",
    "                       token='...' ) #put the token that Mateo sent to you on an e-mail on Wednesday..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**IMPORTANT:** Set the path to data to the right value where you put your (e.g. `\"C:/Users/username/Downloads/\"`)\n",
    "If you put the data files for this workshop in the same directory as the notebook ('.ipynb' file) then, just set this to `\"./\"`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_DIR = \"C:/_DATA/DW3/\" # change to \"./\" if you put the data files next to the notebook file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In this data workshop, we will see some advanced data operatorions using Pandas such as merging, grouping, accessing data through an index, as well as re-arranging data. \n",
    "\n",
    "Let's load our 'House Prices' dataset that we are familiar with from the previous workshop:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_df = pd.read_csv( DATA_DIR + \"house_prices_and_characteristics.csv\" )\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>LotConfig</th>\n",
       "      <th>LandSlope</th>\n",
       "      <th>Neighborhood</th>\n",
       "      <th>Condition1</th>\n",
       "      <th>Condition2</th>\n",
       "      <th>BldgType</th>\n",
       "      <th>HouseStyle</th>\n",
       "      <th>OverallQual</th>\n",
       "      <th>OverallCond</th>\n",
       "      <th>YearBuilt</th>\n",
       "      <th>YearRemodAdd</th>\n",
       "      <th>RoofStyle</th>\n",
       "      <th>RoofMatl</th>\n",
       "      <th>Exterior1st</th>\n",
       "      <th>Exterior2nd</th>\n",
       "      <th>...</th>\n",
       "      <th>Fireplaces</th>\n",
       "      <th>FireplaceQu</th>\n",
       "      <th>GarageType</th>\n",
       "      <th>GarageYrBlt</th>\n",
       "      <th>GarageFinish</th>\n",
       "      <th>GarageCars</th>\n",
       "      <th>GarageArea</th>\n",
       "      <th>GarageQual</th>\n",
       "      <th>GarageCond</th>\n",
       "      <th>PavedDrive</th>\n",
       "      <th>WoodDeckSF</th>\n",
       "      <th>OpenPorchSF</th>\n",
       "      <th>EnclosedPorch</th>\n",
       "      <th>3SsnPorch</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "      <th>SalePrice</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2003</td>\n",
       "      <td>2003</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2003.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>548</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>208500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Veenker</td>\n",
       "      <td>Feedr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>1Story</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>1976</td>\n",
       "      <td>1976</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>MetalSd</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>1976.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>460</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>298</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>181500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Inside</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>CollgCr</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>2001</td>\n",
       "      <td>2002</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2001.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>2</td>\n",
       "      <td>608</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>223500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>Corner</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>Crawfor</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>7</td>\n",
       "      <td>5</td>\n",
       "      <td>1915</td>\n",
       "      <td>1970</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>Wd Sdng</td>\n",
       "      <td>Wd Shng</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>Gd</td>\n",
       "      <td>Detchd</td>\n",
       "      <td>1998.0</td>\n",
       "      <td>Unf</td>\n",
       "      <td>3</td>\n",
       "      <td>642</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>0</td>\n",
       "      <td>35</td>\n",
       "      <td>272</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "      <td>140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>FR2</td>\n",
       "      <td>Gtl</td>\n",
       "      <td>NoRidge</td>\n",
       "      <td>Norm</td>\n",
       "      <td>Norm</td>\n",
       "      <td>1Fam</td>\n",
       "      <td>2Story</td>\n",
       "      <td>8</td>\n",
       "      <td>5</td>\n",
       "      <td>2000</td>\n",
       "      <td>2000</td>\n",
       "      <td>Gable</td>\n",
       "      <td>CompShg</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>VinylSd</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>TA</td>\n",
       "      <td>Attchd</td>\n",
       "      <td>2000.0</td>\n",
       "      <td>RFn</td>\n",
       "      <td>3</td>\n",
       "      <td>836</td>\n",
       "      <td>TA</td>\n",
       "      <td>TA</td>\n",
       "      <td>Y</td>\n",
       "      <td>192</td>\n",
       "      <td>84</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "      <td>250000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 81 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities LotConfig LandSlope Neighborhood Condition1  \\\n",
       "0         Lvl    AllPub    Inside       Gtl      CollgCr       Norm   \n",
       "1         Lvl    AllPub       FR2       Gtl      Veenker      Feedr   \n",
       "2         Lvl    AllPub    Inside       Gtl      CollgCr       Norm   \n",
       "3         Lvl    AllPub    Corner       Gtl      Crawfor       Norm   \n",
       "4         Lvl    AllPub       FR2       Gtl      NoRidge       Norm   \n",
       "\n",
       "  Condition2 BldgType HouseStyle  OverallQual  OverallCond  YearBuilt  \\\n",
       "0       Norm     1Fam     2Story            7            5       2003   \n",
       "1       Norm     1Fam     1Story            6            8       1976   \n",
       "2       Norm     1Fam     2Story            7            5       2001   \n",
       "3       Norm     1Fam     2Story            7            5       1915   \n",
       "4       Norm     1Fam     2Story            8            5       2000   \n",
       "\n",
       "   YearRemodAdd RoofStyle RoofMatl Exterior1st Exterior2nd    ...      \\\n",
       "0          2003     Gable  CompShg     VinylSd     VinylSd    ...       \n",
       "1          1976     Gable  CompShg     MetalSd     MetalSd    ...       \n",
       "2          2002     Gable  CompShg     VinylSd     VinylSd    ...       \n",
       "3          1970     Gable  CompShg     Wd Sdng     Wd Shng    ...       \n",
       "4          2000     Gable  CompShg     VinylSd     VinylSd    ...       \n",
       "\n",
       "  Fireplaces  FireplaceQu GarageType GarageYrBlt GarageFinish GarageCars  \\\n",
       "0          0          NaN     Attchd      2003.0          RFn          2   \n",
       "1          1           TA     Attchd      1976.0          RFn          2   \n",
       "2          1           TA     Attchd      2001.0          RFn          2   \n",
       "3          1           Gd     Detchd      1998.0          Unf          3   \n",
       "4          1           TA     Attchd      2000.0          RFn          3   \n",
       "\n",
       "  GarageArea GarageQual GarageCond  PavedDrive WoodDeckSF  OpenPorchSF  \\\n",
       "0        548         TA         TA           Y          0           61   \n",
       "1        460         TA         TA           Y        298            0   \n",
       "2        608         TA         TA           Y          0           42   \n",
       "3        642         TA         TA           Y          0           35   \n",
       "4        836         TA         TA           Y        192           84   \n",
       "\n",
       "   EnclosedPorch  3SsnPorch ScreenPorch PoolArea PoolQC Fence  MiscFeature  \\\n",
       "0              0          0           0        0    NaN   NaN          NaN   \n",
       "1              0          0           0        0    NaN   NaN          NaN   \n",
       "2              0          0           0        0    NaN   NaN          NaN   \n",
       "3            272          0           0        0    NaN   NaN          NaN   \n",
       "4              0          0           0        0    NaN   NaN          NaN   \n",
       "\n",
       "   MiscVal  MoSold  YrSold  SaleType  SaleCondition  SalePrice  \n",
       "0        0       2    2008        WD         Normal     208500  \n",
       "1        0       5    2007        WD         Normal     181500  \n",
       "2        0       9    2008        WD         Normal     223500  \n",
       "3        0       2    2006        WD        Abnorml     140000  \n",
       "4        0      12    2008        WD         Normal     250000  \n",
       "\n",
       "[5 rows x 81 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "houses_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Group By\n",
    "\n",
    "As its name  implies, this function groups rows of a data frame according to the values in one or more columns.  However, simply calling the function doesn't actually return a `DataFrame` object but rather a `DataFrameGroupBy` object. The latter can be thought of as a dictionary with its keys being the distinct combinations of grouping column values, and each associated value being the group of rows for a column values combination, assembled as a data frame. On a `DataFrameGroupBy` object, one can then apply aggregation operations, such as sum, mean, mode, etc. Keep in mind, that they will only be applied to columns which support such operation, with the other columns being excluded (i.e. non numerical columns being excluded when trying to get the average for each column of the resulting data frame).\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.groupby.DataFrameGroupBy"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by = houses_df.groupby([\"MSZoning\", \"Street\"])\n",
    "type( grp_by  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take a look at the `.groups` attribute of the `grp_by` object. Notice this is just a **dictionary**, with the keys being tuples which are all the different combinations of values from the grouping columns.\n",
    "\n",
    "Don't be scared by `Int64Index` objects! They are just memory efficent ways to collect the indices of rows in each group..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('C (all)', 'Grvl'): Int64Index([812, 1061], dtype='int64'),\n",
       " ('C (all)',\n",
       "  'Pave'): Int64Index([30, 88, 93, 495, 557, 711, 916, 1279], dtype='int64'),\n",
       " ('FV',\n",
       "  'Pave'): Int64Index([  47,   56,   87,  105,  115,  158,  180,  212,  240,  256,  270,\n",
       "              281,  285,  297,  317,  377,  381,  399,  409,  412,  453,  460,\n",
       "              501,  507,  525,  549,  578,  603,  623,  641,  644,  650,  686,\n",
       "              687,  699,  755,  758,  762,  824,  829,  831,  864,  875,  885,\n",
       "              914,  959,  973,  975,  977,  989, 1087, 1089, 1091, 1172, 1191,\n",
       "             1217, 1246, 1265, 1317, 1358, 1364, 1365, 1374, 1442, 1454],\n",
       "            dtype='int64'),\n",
       " ('RH',\n",
       "  'Pave'): Int64Index([ 341,  383,  543,  635,  671,  681,  778,  840,  913,  951,  955,\n",
       "             1030, 1206, 1234, 1264, 1326],\n",
       "            dtype='int64'),\n",
       " ('RL', 'Grvl'): Int64Index([335, 582, 1184], dtype='int64'),\n",
       " ('RL',\n",
       "  'Pave'): Int64Index([   0,    1,    2,    3,    4,    5,    6,    7,    9,   10,\n",
       "             ...\n",
       "             1447, 1448, 1450, 1451, 1453, 1455, 1456, 1457, 1458, 1459],\n",
       "            dtype='int64', length=1148),\n",
       " ('RM', 'Grvl'): Int64Index([52], dtype='int64'),\n",
       " ('RM',\n",
       "  'Pave'): Int64Index([   8,   15,   21,   23,   29,   48,   51,   61,   63,   68,\n",
       "             ...\n",
       "             1400, 1405, 1408, 1416, 1422, 1428, 1438, 1441, 1449, 1452],\n",
       "            dtype='int64', length=217)}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grp_by.groups"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby Exercise 0\n",
    "\n",
    "Take `grp_by` and get the number of rows in the group with key `('RH', 'Pave')`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len( grp_by.groups[ ('RH', 'Pave') ] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_rows = ... # Replace ... with your code\n",
    "ans_submit( 'Grp0', num_rows )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby Exercise 1\n",
    "\n",
    "How many different streets are there in `houses_df`? (suggestion, do some numerical operation on the groupby)c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grp1 = 2\n",
      "Answer for question Grp1 is correct 😃\n",
      "\n"
     ]
    }
   ],
   "source": [
    "num_streets = len(houses_df.groupby(['Street'] )) # Replace ... with your code\n",
    "ans_submit( 'Grp1', num_streets )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby Exercise 2\n",
    "\n",
    "How many different combinations of `Street` and `LandContour` are there in `houses_df`?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_combinations = ... # Replace ... with your code\n",
    "ans_submit( 'Grp2', num_combinations )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applying a user-defined function to each group in a group-by"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The fact that group-by  returns multiple sub-data-frames allows _functions to be applied to each individual group_, rather than on the entire data frame, which is useful when trying to analyze data in different groups without having the results of one group being affected by data from other groups. \n",
    "\n",
    "In the following example, two new columns are added to `grp_by` via an apply function, one of them being the rank (position that a value occupies within a sorted list) and the other being the average price of the top half most expensive houses of each group."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>rank_price</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>6.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  rank_price\n",
       "4      2         1.0\n",
       "5      4         2.0\n",
       "1      5         3.0\n",
       "3      8         4.0\n",
       "0     17         5.0\n",
       "2     23         6.0"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(  { \"price\" : [ 17, 5, 23, 8, 2, 4] } )\n",
    "df\n",
    "df[\"rank_price\"] = df[\"price\"].rank()\n",
    "df.sort_values('price')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>price</th>\n",
       "      <th>rank_price</th>\n",
       "      <th>top_half</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>17</td>\n",
       "      <td>5.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23</td>\n",
       "      <td>6.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>1.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>2.0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   price  rank_price  top_half\n",
       "0     17         5.0     False\n",
       "1      5         3.0     False\n",
       "2     23         6.0     False\n",
       "3      8         4.0     False\n",
       "4      2         1.0      True\n",
       "5      4         2.0      True"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['top_half'] = df['rank_price'] < (len(df) / 2) \n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4    2\n",
       "5    4\n",
       "Name: price, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'][  df['rank_price'] < (len(df) / 2)  ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.0"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['price'][  df['rank_price'] < (len(df) / 2)  ].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_grp( grp ) : \n",
    "    \"\"\"Enrich a group (sub-data-frame within a DataFrameGroupBy) by adding a column that ranks the SalePrices in \n",
    "    it and then compute the avg price of the most expensive half of houses. Notice that this number is going to \n",
    "    be the same accross all rows! The length (number of rows) of the resulting data frame is the same as the input one\"\"\"\n",
    "    \n",
    "    grp['RankPrice'] = grp['SalePrice'].rank()\n",
    "    grp['avg_price_top_half'] = grp[\"SalePrice\"][ grp[\"RankPrice\"] < len(grp) / 2 ].mean()  \n",
    "    \n",
    "    return grp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we pass the function above to the 'higher-order' method `.apply`! \n",
    "This is sort of like running a loop over the groups but in a more elegant, maintanable way,  and without writing `for` or `while`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( grp_by.apply( enrich_grp )    \n",
    "        [[\"SalePrice\", \"RankPrice\", \"avg_price_top_half\"]]\n",
    "        .sort_values( ['avg_price_top_half', 'RankPrice']) ).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that the value of 'avg_price_top_half' is constant within each group but, unfortunately, the group identication information is lost...\n",
    "\n",
    "One way to include in the result is the following 'trick':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_grp_v2( grp ) : \n",
    "    \"\"\"Enrich a group (sub-data-frame within a DataFrameGroupBy) by adding a column that ranks the SalePrices in \n",
    "    it and then compute the avg price of the most expensive half of houses. Notice that this number is going to \n",
    "    be the same accross all rows! The length (number of rows) of the resulting data frame is the same as the input one\"\"\"\n",
    "    \n",
    "    # TRICK to retrieve the v\n",
    "    # Access each component of the name attribute of the group to find out the original values \n",
    "    # of the grouping columns\n",
    "    grp['MSZoning'] = grp.name[0] \n",
    "    grp['Street']   = grp.name[1]\n",
    "    \n",
    "    \n",
    "    grp['RankPrice'] = grp['SalePrice'].rank()\n",
    "    grp['avg_price_top_half'] = grp[\"SalePrice\"][ grp[\"RankPrice\"] < len(grp) / 2 ].mean()  \n",
    "    \n",
    "    return grp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "( grp_by.apply( enrich_grp_v2 )    \n",
    "        [[\"MSZoning\", \"Street\", \"SalePrice\", \"RankPrice\", \"avg_price_top_half\"]]\n",
    "        .sort_values( ['avg_price_top_half', 'RankPrice']) ).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Groupby Exercise 3\n",
    "\n",
    "Redo the same grouping of the previous exercise but instead of getting the sum of lot areas, create a function that gets the _second_ largest lot area in the group.  In case the group has only 1 row, return float(\"NaN\")\n",
    "\n",
    "** Hints: **\n",
    " * You can sort a series (column), such as `grp['LotArea']` in descending order by calling `.sort_values(ascending=False)` on it. \n",
    " * You can find out the number of rows in a df (or grp) by calling `len()` on it (this is a function, not method) \n",
    " * You can get the second element of a (sorted) series `ser`  with the syntax `ser.iloc[1]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ... with your code \n",
    "def second_largest( grp ) :\n",
    "    if len( grp ) > 1 : \n",
    "        sorted_areas =  grp['LotArea'].sort_values( ascending=False )\n",
    "        return sorted_areas.iloc[1]\n",
    "    else :\n",
    "        return float( \"NaN\" )\n",
    "\n",
    "serG3 = houses_df.groupby( [\"MSZoning\", \"Street\"] ).apply( second_largest )  \n",
    "serG3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "252766.0"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "serG3.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( 'Grp3', int(serG3.sum()) )  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Aggregation\n",
    "\n",
    "The aggregation method `.aggregate` (_alias_: `.agg`)  in the class `DataFrameGroupBy` allows multiple aggregation operations to be performed on all or a subset of the columns of data frame\n",
    "\n",
    "This functions admits two ways of calling it: \n",
    "\n",
    "In the first one *a list of aggregation function names* (strings) is passed to it.  This results in all those aggregation functions being applied to all columns of the dataframe (where the aggregation functions applies):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_by.aggregate([\"sum\", \"mean\"])   # effectively the same as houses_df.groupby( ... ).aggregate( [\"sum\", \"mean\"] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The operations are only applied on columns which allow them, while the rest of columns are simply ignored from the output. In the following example it can be seen how `aggregated` ends up having less columns that the data frame it came from, `houses_df`, due to the latter having columns which don't allow numerical operations to be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aggregated = houses_df.groupby([\"MSZoning\", \"Street\"]).mean()\n",
    "print(\"houses_df has %d columns\\naggregated has %d columns\" % (houses_df.shape[1],aggregated.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A second way of calling it is passing a dictionary in which the keys are data frame columns and the values are the names of aggregation functions to apply to each column and on each group, separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grp_by.agg({ \"LotArea\"     : \"mean\",\n",
    "             \"SalePrice\"   : \"sum\",\n",
    "             \"MSSubClass\"  : pd.np.median,    \n",
    "             # Series.nunique() computes the number of unique values in a series. \n",
    "             \"MiscVal\"     : lambda ser : ser.nunique(),\n",
    "             # Series.value_counts() returns dict of counts of different values\n",
    "             \"Exterior1st\" : lambda ser : \" | \".join( ser.value_counts().keys() )   }) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to aggregate after group-by, without calling `.aggregate`, one can directly call a particular aggregation (e.g. `.sum()`, `.mean()`, `.count()` and several others) method to all 'compatible' compatible columns. The advantage / disadavantage of is that the same method and only that one is applied to all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_df.groupby([\"Street\", \"LandContour\"]).count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**And now for the actual aggregation exercises, in which the output data does matter**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation Exercise 1\n",
    "\n",
    "Group `houses_df` by `Street` and `LotShape` and sum only the lot area of each group"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ser0 = ... # Replace ... with your code\n",
    "ser0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( 'Agg1', ser0.loc[('Grvl', 'Reg')]['LotArea'] )  #In a section below we explain what .loc does."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introducing the Gowalla check-ins dataset\n",
    "\n",
    "In the following examples we shall work on (a subset of) the Gowalla check-ins dataset. \n",
    "\n",
    "This dataset contains check-in events by users of the Gowalla social networking site to set of locations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_df = ( pd.read_csv( DATA_DIR + \"check_in_gowalla.csv\")                  \n",
    "                  .rename( columns={\"check_in_time\" : \"checkin_time\", 'location_id' : \"location\" })\n",
    "                  .sort_values( 'location' ))\n",
    "\n",
    "checkins_df.head(10) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation Exercise 3\n",
    "\n",
    "Use `checkins_df` to get the number of visits and datetime of the first visit for each location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg3_df = ... # replace ... with your code \n",
    "ans_submit( 'Agg31',agg3_df['user'].sum())\n",
    "ans_submit( 'Agg32',agg3_df['checkin_time'].loc[9101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Aggregation Exercise 4\n",
    "\n",
    "Use `checkins_df` to compute the number of unique (different) visitors per location (suggestion: search for the `nunique` method), for all locations at once!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg4_df = ...  # replace ... with your code \n",
    "\n",
    "ans_submit( 'Agg4',agg4_df['user'].loc[9101])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Indexing\n",
    "\n",
    "An **index** in a data frame is a set of values that play essentially the same role as the keys in a dictionary. An index is usually composed of integers, strings or tuples. To each value of the index, there is an associated row or, possibly, a *set of rows* in the data frame. \n",
    "\n",
    "Further, accessing that row or set of rows is an efficient (O(1)) operation that *does not* require to scan the whole data set. \n",
    "\n",
    "It's important to note that the index is *not considered* a column in the dataset as such. It's in a different category. However, an index can be easily turned into a column as we will see below.\n",
    "\n",
    "### set_index\n",
    "\n",
    "Every data frame has an index. When the dataframe is first created from scratch, the index keys are just integers ranging from 0 to `len(df)-1`. \n",
    "\n",
    "However it is often more useful to define and index from the values of one or several columns column.\n",
    "This is done by means of the `.set_index` method (in the `DataFrame` class)\n",
    "\n",
    "This method makes turns the specified column into the new index of the data frame by either taking it out of the data columns (default) or keeping it both as the index as well as a data column (by setting the drop parameter to False). More than one column may be set as index by inputing a list of columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember the `checkins_df` looks like"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_df.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that there is a column of apparently random numbers  on the let that is not named. That's actually not a column, but the index. Notice that, when we defined the dataframe, we sorted it by `location` right after loading it from the csv. The first record in the resulting order by `location` actually comes from the 826-th row of the `csv`. That's where the 826 in front of the first record comes from.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the `.index` attribute we see that `checkins_df` has an index consisting of integers, in the order just shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_df.index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_1 = checkins_df.set_index( 'location' )\n",
    "chins_indexed_1.head( 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_1.index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, the index consists of integers but they come from `location` column in the original data frame."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It's important to see that setting an index on a data frame doesn't actually change the original data frame at all, rather it creates a new one with the specified index. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkins_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not let's define and index on two columns!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_2 = checkins_df.set_index( ['location', 'user'] )\n",
    "chins_indexed_2.head( 20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The resulting index is called a _hierarchical_ index because it has a hierarchy of levels. \n",
    "\n",
    "In this case , the first level groups  records by location an the second by user."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise I0: ** \n",
    "\n",
    "The following line generates an error. Copy an paste the *last* (non-empty) line of the error message into the answer. \n",
    "Make sure to understand **why** this error is produced. If you don't understand, discuss it with your instructor. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_1['location']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( \"I0\", \"copy and paste the last (non-empty) line of the error message\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also possible to keep a column as data as well as setting it as and index by  passing the `drop = False` to `set_index`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chins_indexed_1 = checkins_df.set_index(\"location\", drop=False)\n",
    "chins_indexed_1.head( 20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieving rows through an index -- basic usage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The primary purpose of an index is to *efficiently* retrieve a row or set of rows from a data frame. This is done throught the `DataFrame.loc` accessor ( 'loc' is short for 'locate' ). We will go in depth into the usage of `loc`, but for now the essential usage is as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row = checkins_df.loc[826]\n",
    "first_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise I1: ** \n",
    "\n",
    "What is the _type_ of `first_row`?  (You can use the built-in function `type()` to answer this question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( \"I1\", \"pandas.....fill-in rest of the type (fully-qualified) name\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "first_row_v2 = checkins_df.loc[ [826] ]\n",
    "first_row_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as before except that it is nicely formatted (why?)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise I2**\n",
    "\n",
    "What is the type of `first_row_v2`? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( \"I2\", \"pandas.....\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "another_row = checkins_df.loc[ [0] ]\n",
    "another_row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that a *single* index value can map to _many_ rows..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_1.loc[1038401]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise I3 **\n",
    "\n",
    "How many rows are there for location = 9501"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( \"I3\",...  ) # your answer instead of ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accessing rows when there is a hierarchichal index \n",
    "\n",
    "When there are _n_ levels in the index, you can specify 1, 2 or up to 'n' values in to the `.loc` accessor "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_2.loc[ 9101 ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_2.loc[  (9101, 346 ), ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to avoid that nasty warning we are careful to sort both levels of the index.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_2s = chins_indexed_2.sort_index( level= [0,1], axis=0)  \n",
    "chins_indexed_2s.loc[ (9101,346) ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### reset_index\n",
    "\n",
    "It reset the index to an auto-incremental one and makes any previously set index a data column again. By default it resets all previously indexed columns, but the parameter level allows only certain indexes to be reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_2.reset_index().head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_indexed_2.reset_index(level=1).head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise R0\n",
    "\n",
    "Reset the index of `chins_indexed_2s` by one level and count how many different users have been on location 9101"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r0_ans = ...  # replace ... with your answer\n",
    "ans_submit( 'R0', r0_ans  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## iloc and loc\n",
    "\n",
    "Pandas' main way of referencing data is through `.iloc` and `.loc` _accessors_ (which are special types of attributes)\n",
    "\n",
    "Both accesors are similar but only superficially so.\n",
    "\n",
    "`iloc` is simpler, it's basic usage syntax is as follows\n",
    "\n",
    "```\n",
    "   df.iloc[ list_of_ints_r, list_of_ints_c ]  \n",
    "```\n",
    "\n",
    "Here `list_of_ints_r` specifies the 0-based _integer indices_ of _rows_ in the dataframe's own order and and `list_of_ints_c` specifies the 0-based indices of `columns`. Despite its name `iloc` does not take into account the dataframe's index at all! The `i` in `iloc` stands for `integer`. \n",
    "\n",
    "Instead of `list_of_ints_r`, you can also put a slice, such as `10:47` to get rows numbered 10 through 46 (not including 47 as is usual with Python slicing). Remember that just writing `:47` is a shorthand for `0:47`  and `10:` is a shorthand for `10:len(df)`. Similarly just writing `:` means `0:len(df)`. \n",
    "\n",
    "Analogous considerations hold for the second argument, `list_of_ints_c`. \n",
    "\n",
    "\n",
    "Now onto `loc`.\n",
    "\n",
    "The `.loc` method (accessor)  receives as input list of row keys and another one column names.\n",
    "\n",
    "Row keys are the keys defined in the index. The colum names are the regular column names you already know and love. \n",
    "\n",
    "\n",
    "The basic syntax is:\n",
    "\n",
    "```\n",
    "   df.loc[ list_of_row_keys, list_of_col_names ]  \n",
    "```\n",
    "\n",
    "It is also possible to  specify the _row keys_ only:\n",
    "\n",
    "```\n",
    "   df.loc[ list_of_row_keys ]  \n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_df2 = houses_df.set_index(\"Neighborhood\")\n",
    "houses_df2.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_df2.iloc[ [1,2,3], [0, 1,3] ]  # just indexing by the order they appear in the df. 0-based indexing!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_df2.iloc[ 1:7, 0:5 ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we use `.loc` with only row keys: \n",
    "\n",
    "Notice that a **single key can refer to many rows**! (and this is not unusual...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_df2.loc[ [\"Blueste\",\"NPkVill\"] ].head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_df2.loc[ [\"Blueste\",\"NPkVill\"], ['LotArea', 'Street'] ].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn index back into a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_df2 = houses_df2.reset_index()\n",
    "houses_df2.head( 10 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot\n",
    "\n",
    "The pivot function generates a new dataframe rearranging a column from a given data  frame in the form of a grid, which may be clearer to analyze. \n",
    "\n",
    "The function receives three basic arguments:\n",
    "\n",
    "  * **index**: specifies the column whose values will become the index of the new data frame.\n",
    "  * **columns** : a column whose values are going to be turned into column headers of the new data frame. It is advisable for this particular column not to have many different values so that the new data frame won't be too wide (have a lot of columns).\n",
    "  * ** values ** : A column whose values are going to be displayed in the grid. Although a list of columns can be used for values , doing so would be the same as simply pivoting 2 different values and concatenating the result.\n",
    "  \n",
    "All this will be a lot clearer with an example. \n",
    "\n",
    "First we generate an aggregated version of our houses data frame to get average prices and areas for all the various combinations of Exterior1st and Street."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_agg = ( houses_df.groupby([\"Exterior1st\", \"Street\"])\n",
    "                        .agg( {\"SalePrice\" : \"mean\", \"LotArea\" : \"mean\"})\n",
    "                        .reset_index()\n",
    "                        .sort_values('Street')) \n",
    "houses_agg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that only some of the values of 'Exterior1st' appeared for `Street = 'Grvl'`, hence the grid contains NaNs for those.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_agg.pivot(index='Exterior1st', columns=\"Street\", values='SalePrice').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pivot Exercise 0\n",
    "\n",
    "Take the houses_df data frame, group it by MSZoning and LotShape, get the average SalePrice and pivot it with LotShape and MSZoning as index and columns respectively (to use either LotShape or MSZoning to pivot, you have to reset the index of the group by)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ... with your code :\n",
    "houses_agg = ...\n",
    "df = ...  \n",
    "df "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( 'Piv0',df.iloc[1,2]  )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Merge and join\n",
    "\n",
    "Both functions allow data from different data frames to be combined into a single one according to a 'crossing' or 'look-up' rule. \n",
    "\n",
    "Although both `merge` and `join` do really a similar things, the way they do them is different. \n",
    "\n",
    "The function `merge` is pandas default function for joining data. It's basically *pandas counterpart of SQL's join*, and requires the specification of which columns of both data frames would be compared. Merge doesn't care at all about about the indexes defined on them. \n",
    "\n",
    "On the other hand, Panda's `join function is more of a convenience thing (it even uses merge internally), joining is basically doing a merge by taking advantage of the indexes of both data frames. \n",
    "\n",
    "The following figure summarizes the different 4 types or merge: _ inner, outer, left and right_.\n",
    "\n",
    "The function merge is also availaible as a method in the  `DataFrame` class. \n",
    "The basic syntax is:\n",
    "\n",
    "```\n",
    "new_joined_df = df.merge( another_df, left_on = \"col_in_df\",  right_on = \"col_in_another_df\",\n",
    "                          how=\"inner\"|\"left\"|\"right\"|\"outer\" ) \n",
    "```\n",
    "\n",
    "The first argument (`another_df`) as well as `left_on` and `right_on` are required arguments. \n",
    "`left_on` specifies a column name on the data frame `df` whose values should be matched with \n",
    "those of the  `another_df`'s column specified by `right_on` in. \n",
    "\n",
    "The `how` argument is optional ans specifies the type of join:\n",
    " \n",
    " <img src=\"merge.png\" height=\"200\" width=\"800\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see some examples, we load some (financial!) data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdcop = pd.read_csv( DATA_DIR + 'usdcop.csv', delimiter = \"\\t\", infer_datetime_format=True)\n",
    "usdcop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdcop = pd.read_csv( DATA_DIR + 'usdcop.csv', delimiter = \"\\t\", infer_datetime_format=True)\n",
    "usdcop['usd_cop'] = usdcop['usd_cop'].str.replace('$', '').str.replace(',', '').astype( float )\n",
    "usdcop['fecha'] = ( usdcop['fecha'].str[6:10] + '-' + usdcop['fecha'].str[3:5] + '-' + usdcop['fecha'].str[0:2] )\n",
    "usdcop  # Now contains the exchange rate from usd to cop  as type float!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btcusd = pd.read_csv( DATA_DIR + 'btcusd.csv' ) # this file contains hourly exchange rate from BTC (bitcoin) to USD\n",
    "btcusd['date'] = btcusd['date_tm'].str[0:10]\n",
    "btcusd['time'] = btcusd['date_tm'].str[11:]\n",
    "\n",
    "btcusd.head(10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "btcusd_day_max = btcusd.groupby('date').agg( {'btc_usd' : 'max' } ).reset_index()\n",
    "btcusd_day_min = btcusd.groupby('date').agg( {'btc_usd' : 'min' } ).reset_index()\n",
    "btcusd_day = pd.concat( [btcusd_day_max, btcusd_day_min] ).sort_values( 'date' )\n",
    "btcusd_day  # Now contains both the minimum and maximum exchange rates between btc (bitcoin) and USD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inner merge\n",
    "\n",
    "Is the default merge in case the `how` parameter is not specified. It yields rows for which there are matching values of the specified merge columns on both data_Frames."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdcop.merge( btcusd_day_min, left_on='fecha', right_on='date' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question:** Why did the first `btc_usd` price (7930.79) get duplicated?  If you don't understand it, discuss it with your instructor! "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdcop.merge( btcusd_day, left_on='fecha', right_on='date' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Left merge\n",
    "\n",
    "It gkeeps all the data from the first data frame, adding data from the second one whenever there is a row matching and filling with `NaN` the missing columns from the second data frame in which no match was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdcop.merge( btcusd_day_min, how=\"left\", left_on='fecha', right_on='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Right merge\n",
    "\n",
    "It's pretty much the same exact thing as left merge with the data frame on the left taking being on the right and viceversa. It keeps all the data from the second data frame adding data from the first one whenever they intersect and filling with `NaN` the missing columns from the first data frame if no match was found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdcop.merge( btcusd_day_min,  how=\"right\", left_on='fecha', right_on='date').sort_values( 'date' )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outer merge\n",
    "\n",
    "It's basically the combination of both left and right join, keeping all the data from both data frames and filling out with NaN if no match found was found for either."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "usdcop.merge( btcusd_day_min, how=\"outer\", left_on='fecha', right_on='date')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Exercise 0 \n",
    "\n",
    "Compute the outer merge between the `usdcop` and `btcusd_day` data frames on the `fecha`/`date`. Call the resulting data frame `buc_min`. To this data frame, add a new column `btc_cop` containing the exchange rate from `btc` to `cop`. This can be computed as the product of the two available exchange rates (`btc_usd`, `usd_cop`). \n",
    "\n",
    "Now generate a new series `agg` by applying `groupby('date')` to `buc_min` and compute both  the mean and the median of the new column for each `date`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace ... by your code \n",
    "buc_min = ... \n",
    "...\n",
    "agg = buc_min.groupby('date').agg( {\"btc_cop\" : [\"mean\", \"median\"]} ) \n",
    "\n",
    "ans_submit( 'Mer0', int(agg[('btc_cop', 'median')].sum()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Join\n",
    "\n",
    "Sometimes, when data frames are already indexed by the same 'thing', it is a lot easier to use `join`, which uses matches rows from two them according to their index value. \n",
    "\n",
    "To see this, let's define two dataframes indexed by 'location' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counts_by_loc = ( checkins_df.groupby( \"location\" )\n",
    "                             .agg( { \"user\" : \"nunique\",\n",
    "                                     \"checkin_time\" : \"count\"}) )\n",
    "counts_by_loc.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lat_lon_by_loc = ( checkins_df[['location', 'latitude', 'longitude']]\n",
    "                              .drop_duplicates()\n",
    "                              .set_index('location') ) \n",
    "lat_lon_by_loc.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As the both dataframes are indexed on the same id, it is very easy to just join them by that id."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_by_loc.join( lat_lon_by_loc ).head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A non so conventional way of merging data is by merging a data frame with itself as to compare some rows with one another, here's an example of it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A non-conventional merge: self-merge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins2 = checkins_df.merge( checkins_df, on=\"location\" )\n",
    "chins3 = chins2[ (chins2.checkin_time_x < chins2.checkin_time_y) & \n",
    "                 (chins2.user_x != chins2.user_y )]\n",
    "chins4 = ( chins3[[\"user_x\", \"user_y\", \"location\"]]\n",
    "                 .drop_duplicates()                 \n",
    "                 .groupby([\"user_x\", \"user_y\"])\n",
    "                 .agg( {\"location\" : \"count\"})\n",
    "                 .rename( columns = { \"location\" : \"location_count\" } )\n",
    "                 .reset_index() )\n",
    "\n",
    "chins4.sort_values('location_count', ascending=False).head(40)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we merged the checkins data frame with itself every time an user went to a location some other user had gone as well. We then filter the rows in which an user arrived after another one, not at the same time (given that the previous merge rows matched with themselves). Finally the information was grouped for a better visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge Exercise 1\n",
    "\n",
    "Perform an inner merge between the first 100 rows and the following 100 rows of `houses_df` comparing the `Neighborhood` column.\n",
    "Call the result `houses_df_self_join`. How many rows does this df have?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "houses_df_self_join = ... # replace ... with your code \n",
    "ans_submit( 'Mer1', ... )   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift\n",
    "\n",
    "The shift function is pandas' way of lagging a series, which is useful when trying to see how something changes as time goes on, like predicting a future value for certain series, or studying how a given series affects another one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chins_u4771 = checkins_df[ checkins_df.user == 4771 ].copy()\n",
    "chins_u4771.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "chins_u4771['checkin_tm_prev'] = chins_u4771['checkin_time'].shift(1)\n",
    "chins_u4771['checkin_tm_next'] = chins_u4771['checkin_time'].shift(-1)\n",
    "chins_u4771.head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the previous example the shift function was really poorly used, given that by simple shifting every register by one position the first register for every user (apart from user 0) is literally the last register from the previous user which is obviously false information. To avoid doing so, it would be best to first group up the data by user, that being said..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shift Exercise 0\n",
    "\n",
    "Shift only the check in time of every user in the `checkins_df` by 1 position (as a sugestion do a groupby and apply the same technique as with enrich_grp function above...) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = ... # replace ... with your code \n",
    "ans_submit( 'Shift0', math.isnan(df.iloc[2,-1])  )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
