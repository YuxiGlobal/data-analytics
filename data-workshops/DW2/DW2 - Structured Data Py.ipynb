{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Before you start: Create the submitter function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append( '../')\n",
    "import dw_utils2 \n",
    "\n",
    "ans_submit = dw_utils2.create_submitter(host='52.91.20.10', port=80, \n",
    "                       user=\"mateo.restrepo@yuxiglobal.com\", \n",
    "                        # put your full yuxi email address here, including @yuxiglobal.com\n",
    "                       ws_key=\"dw3\", #this is the workshop key, don't change it \n",
    "                       token=\"1nwK1Z4kN1Rk\" ) #put the token that Mateo sent to you on an e-mail on Wednesday..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Agg1 = ''\n",
      "Answer for question Agg1 is incorrect ☹\n",
      "\n"
     ]
    }
   ],
   "source": [
    "ans_submit( \"Agg1\", \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is \"data\"?\n",
    "\n",
    "Data is any collection of (true) **facts of statements** about **objects** (or entities) in some domain. Traditionally, data talked about objects in nature, society or the \"real\" world in general.  Lately, there is an increasing amount of data that refers to objects in the \"virtual\" world, such as web transactional events, chats, user profiles, etc...\n",
    "\n",
    "Many times, these facts or statements take the form of definite values for certain **attributes** of the objects in question. \n",
    "\n",
    "**Examples:** \n",
    "\n",
    "  * The objects are 'users' of an app, and their attributes are name, date of birth, e-mail, hair-color, and so on...\n",
    "  * The objects are 'events' in a server, and their attributes are the time, whether the event was user generated or not, whether they were an error or not, the user associated with the event (if any)\n",
    "  \n",
    "Attributes are no more than (partial) **functions** that map entities to other (often times simpler) data types, such as ´int´, ´double´, ´string´ or ´bool´, etc...\n",
    "\n",
    "$$ \\mathtt{name} : \\mathtt{User} \\rightarrow \\mathtt{String} $$\n",
    "\n",
    "$$ \\mathtt{date\\_of\\_birth} : \\mathtt{User} \\rightarrow \\mathtt{Date} $$\n",
    "\n",
    "$$ \\mathtt{event-time} : \\mathtt{ServerEvent} \\rightarrow \\mathtt{DateTime}  $$\n",
    "\n",
    "$$ \\mathtt{is-user-generated} : \\mathtt{ServerEvent} \\rightarrow \\mathtt{Bool}  $$\n",
    "\n",
    "$$ \\mathtt{hair\\_color} : \\mathtt{User} \\rightarrow \\mathtt{Color} \\equiv {\\mathbb{R}^3} $$\n",
    "\n",
    "\n",
    "  \n",
    "## Kinds of data \n",
    "\n",
    "Data can be roughly classified in one of three kinds. \n",
    "\n",
    "* **Structured data**: Data that can **easily** be put in tabular form that makes processing it convenient. Structured data consists of a multitude of identically formatted data elements (records) each having roughly the same set of (relatively simple) attributes that capture most of what is relevant about them. Much more on this below.  \n",
    "\n",
    "* **Unstructured data** Data that is hard to put in tabular form without incurring in either significant loss or affecting the ease of processing.  Examples: a collection of texts written by humans, a collection of high-resolution photos, audio recording, videos, x-ray.\n",
    "\n",
    "* **Semi-structured data** Data for which can be partly but not wholly put in tabular form.  Examples: web logs, chats, tweets, maps. \n",
    "\n",
    "\n",
    "**Note:** Often times working with ustructured or semi-structure data starts with converting it into an structured form (tabular). \n",
    "\n",
    "\n",
    "## Structured Data: an executive summary\n",
    "\n",
    "In what follows we shall focus on structured data. In future workshops we will revisit semi-structured and structured data. \n",
    "\n",
    "Recall that structured data can be represented faithfully in **tabular form**, and for which such as representation is convenient for processing.\n",
    "\n",
    "A structured data-set consists of many **records**. Each record holds data about one particular instance of an entity (or object) type. A record consists simply of values for a fixed set of attributes for that instance. In principle, the **set of attributes** is fixed and common accross all records in the data set; it is the values for those attributes that vary. \n",
    "It is not uncommon for some attributes to be undefined for some, sometimes many, of the objects.\n",
    "\n",
    "We say then that, all records have the same \"shape\", since they all contain the **same attributes** and the **type** (set of possible values) **of each attribute is fixed**.\n",
    "\n",
    "When represented in **tabular** form, it is usual to refer to a dataset simply as a **table**. We will use both terms **dataset** and **table** interchangeably.  Similarly, in the usual tabular representation, **records** become **rows*** and each attribute becomes a **field**. A **column** then refers collectively to all values for a particular field across all records of a given table. \n",
    "\n",
    "The purists of DB theory, sometimes say **relation** to mean table. The technical term actually comes from [math](https://en.wikipedia.org/wiki/Finitary_relation)..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of structured data: \n",
    "\n",
    "The following data comes from the didactic competition [House Prices: Advanced Regression Techniques](https://www.kaggle.com/c/house-prices-advanced-regression-techniques)\n",
    "\n",
    "First, we import the `pandas` module (=library) and alias it as  `pd` (this is a good and very common practice with Python), as it contributes to making code more concise without sacrificing readability.   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "print( \"Pandas version is: \", pd.__version__, \n",
    "       \"  Keep this in mind when looking at the API docs!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we use the function `read_csv` to load the data and return it as an object of class DataFrame.\n",
    "Most likely, you will have to change the path in the call to the function. Remember to use `/` to separate paths, instead of \n",
    "`\\` which is used to escape special characters (just like in C). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv( \"C:/_DATA/Data Workshops/DW2/house_prices_and_characteristics.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type( df ) # this yields the type of the object. In Python every type is a class (I think...)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df  # this attempts to display the contents of the DataFrame object in an html-friendly way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Notice:**\n",
    "  * The first column, containing the numbers from 0 to 1459 in boldface, is not actually a field, but an **index** of the rows. That means that these values can be used to refer to individual rows. In other data Frames, the index, will consists of keys of a different type, for example, strings or tuples, and not just mere numbers.\n",
    "  * The visualization above only shows the first few columns, then ..., then the last few columns. Similarly for the rows. The number of rows and columns shown is configurable, but one rarely does configure it.\n",
    "  * Some columns contain the special value 'NaN' (Not-a-Number) in some of the entries. 'NaN' is actually a special value of the type float (64-bit floating numbers, called 'double' in C/C++/Java, etc.). NaN is obtainable evaluating the expression `float(\"NaN\")`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Motivational plot\n",
    "\n",
    "With pandas, it's very easy to create a quick plot of one column againts another...\n",
    "\n",
    "**NOTE**: If the following doesn't show after evaluating it once "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#**NOTE**: If this cell doesn't show after evaluating it once, just run it a second time\n",
    "\n",
    "df.plot.scatter( x=\"GrLivArea\" , y=\"SalePrice\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A note on missing values in Python/pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_a_number = float( \"NaN\" )\n",
    "not_a_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type( not_a_number )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ACHTUNG!** `nan` does not equal any other value, not even itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not_a_number == not_a_number "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To appropriately check whether something is nan one way is to use the `isnan` function from the math module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.isnan( not_a_number )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.isnan( 2 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "math.isnan( \"nan\" ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Questions: \n",
    "\n",
    "  * **(X1)** In the dataset above, what is the name of the first column (going from left to right) that contains 'NaN' values? \n",
    "  * **(X2)** What is the name of the first non-numeric column (going from left to right) that contains 'NaN' values?\n",
    "  * **(X3)** Look at the last line in the output above, what is the total number of records (rows)? \n",
    "  * **(X4)** What is the total number of columns?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( \"X1\", \"...your answer here...\") \n",
    "ans_submit( \"X2\", \"...your answer here...\") \n",
    "ans_submit( \"X3\", \"...your answer here...\") \n",
    "ans_submit( \"X4\", \"...your answer here...\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you can get the full list of column names for the DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To get columns and their data types you can access the `dtypes` field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice how fields of type string appear as having type `object`. This means each data cell in the table only holds a *reference to* the actual object that is an immutable string stored somewhere else. For strings that are variable length this is the only way to \"store\" them on a table. Thus accessing a cell value that contains an object, implicitely requires doing extra dereferencing operations and is a bit slower than accessing an **int64** or **float64**.  For fixed length strings there are other alternatives that are *faster* but at the same time might make the memory requirements of the whole dataframe grow.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question X5:** Besides `int64` and `object`, what is another data type that appears in the list above?  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( \"X5\", \"...your answer here...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 10 minutes to Pandas video"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Take your time to go over the 10 minutes to Pandas video, available on this link: https://vimeo.com/59324550\n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data operations\n",
    "\n",
    "There are a few data operations on data sets that any data-processing tool should implement. \n",
    "\n",
    "These could be roughly classified in the following categories:\n",
    "\n",
    "  * **Enriching (or Transforming)**  a data set, by adding newly calculated columns on indices.\n",
    "  * **Filtering** picking a subset of the rows or columns of a data set according to some criterion.\n",
    "  * **Indexing** adding indices to a data set\n",
    "  * **Aggregating**\n",
    "  * **Sorting** sorting the rows of a dataset according to some criteriong\n",
    "  * **Merging** merging to data sets in some way. This includes: concatenation (horizontal or vertical) and also joining.\n",
    "  * **Summarizing** computing summaries of a data set.\n",
    "  * **Pivoting**: This includes transposing and doing other operation so that data that originally had a vertical layout, is laid out horizonatally (increasing the number of columns) or viceversa (increasing number of rows). The video above has a few cool examples to better understand this concep. \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retrieving one or more columns "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before talking about eriching to refer to a column we use \"diccionary access\" style notation, i.e. **square brackets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LotArea'] # this accesses the data from a column and also shows the corresponding row index..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type( df['LotArea'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice this is no longer a `DataFrame` object but a `Series` object.\n",
    "\n",
    "`Series` is just the pandas core implementation of the concept of column or field.\n",
    "\n",
    "A `Series` is a one-dimensional object.\n",
    "\n",
    "The reason for the name is that pandas was initially designed to handle time-series data (financial stock price series, indexed by time). So the name is more or less a historical accident... "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also choose a subset of columns in the dataframe by using the notation: `df[ list-of-Columns ]`, where a list literal or any expression that yields a list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[ ['Utilities', 'YearBuilt', 'TotRmsAbvGrd'] ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enriching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest way to enrich a data set is to do a *row-by-row* computation involving one or more of its columns as inputs. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, we can transform the `LotArea` from square feet to square meters and store the result in a new columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LotArea_m2'] = df['LotArea'] * (0.3048 * 0.3048)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that `LotArea_m2` now appears as the last column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LotArea_m2'].head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise E1 ** \n",
    "\n",
    "Convert the **First floor area** from square feet to square meters and submit the sum of the resulting column. Refer to the \"metadata\" file to identify the column that stores this attribute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here to define a new column\n",
    "\n",
    "\n",
    "\n",
    "ans_submit( 'E1',  round( df[\"new column\"].sum(), 3 ) ) # Only change the column name in quotes!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise E2 ** \n",
    "\n",
    "Convert the `LotFrontage` to meters and submit the sum of the resulting column (Hint: the correct conversion factor is not 0.3048 * 0.3048 but something simpler.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code here to define another new column\n",
    "\n",
    "\n",
    "\n",
    "ans_submit( 'E2', round(df[\"another column\"].sum(), 3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The class `Series` has a few methods that can transform one column by itself. The most important one is perhaps `.isna()` which generates a new `Series` with data-type Boolean having `True` in at the indices where the original `Series` is `NaN`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LotFrontage_isna'] = df['LotFrontage'].isna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['LotFrontage', 'LotFrontage_isna']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To see all methods offered by the class `Series` go here: https://pandas.pydata.org/pandas-docs/stable/generated/pandas.Series.html \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For columns of type string there is a special attribute called `str` that gives access to most methods of the string class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SC_len'] = df['SaleCondition'].str.len()\n",
    "df['SC_first_chr'] = df['SaleCondition'].str[0]\n",
    "df['SC_upper'] = df['SaleCondition'].str.upper() \n",
    "df['SC_lower'] = df['SaleCondition'].str.lower() \n",
    "df['SC_concat_LS'] = df['SaleCondition'] + '_' +  df['LotShape'] \n",
    "\n",
    "df['e3_input'] = (df['SaleCondition'] + '_' +  df['LotShape']).str.lower()\n",
    "\n",
    "df[ ['SaleCondition', 'SC_len', 'SC_first_chr', 'SC_upper', 'SC_lower', 'LotShape', 'SC_concat_LS', 'e3_input']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise E3 ** \n",
    "\n",
    "The following doesn't work as expected (in each value we want 'a' replaced by '@' and then each 'r' replaced by '', i.e. remove the r's). \n",
    "Figure out why an fix it! (**Hint:** What is the type of the subexpression `df['e3_input'].str.replace('a', '@')` )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['e3_output'] = df['e3_input'].str.replace('a', '@').replace( 'r', '' )  # Fix this....\n",
    "df['e3_output'].head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( 'E3' , df['e3_output'].str.len().sum() ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also posible to do vectorized operations between columns. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"sum_of_areas\"] = df['TotalBsmtSF'] + df['1stFlrSF'] + df['2ndFlrSF']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is computing the sum of columns TotalBsmtSF, 1stFlrSF and 2ndFlrSF on a **row by row basis**.\n",
    "\n",
    "Convince yourself that this is the case by checking a few of the rows below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'sum_of_areas']].head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise E4**\n",
    "\n",
    "Redefine \"sum_of_areas\" to include the garage area in the sum as well (refer to metadata.txt file to find out which column that is)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your code to redefine column \"sum_of_areas\" here \n",
    "\n",
    "\n",
    "\n",
    "ans_submit( \"E4\", df[\"sum_of_areas\"].sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally you can apply any function (built-in, from a library and user-defined) to a series in a row-by-row fashiong by means of the `apply` method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_quadratic( x ) : \n",
    "    \"\"\"This computes a quadratic of a single value\"\"\"\n",
    "    return 3 * (x**2) - 200 * x  -8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LotFrontage_quad'] = df['LotFrontage'].apply( compute_quadratic )\n",
    "df[ ['LotFrontage', 'LotFrontage_quad']].head( 15 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Keep in mind that doing `apply` is essentially the same as going through the series with a for-loop at *Python speed*, which is **very slow**.  \n",
    "\n",
    "Any of the other operations we saw above also do a loop but inside the library code, so they happen at *C speed*. \n",
    "\n",
    "So, whenever possible you should avoid `apply` and make use of the vectorized operations that are available. \n",
    "\n",
    "For instance, the opperation above could have been done as follows.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = df['LotFrontage']  # alias the column for shorter code in the next line (this doesn't do a copy!)\n",
    "df['LotFrontage_quad2'] = 3 * (xs ** 2) - 200 * xs - 8   # this does fast vectorized operations at C-speed!\n",
    "\n",
    "df['equality_test']   = (df['LotFrontage_quad'] == df['LotFrontage_quad2'] )  # compare to the result previously obtained\n",
    "\n",
    "# refined equality test: if LotFrontage is NaN then LotFrontage_quad and LotFrontage_quad2 have to be NaN as well\n",
    "df['equality_test_2'] = df['equality_test']  | (df['LotFrontage'].isna() &  df['LotFrontage_quad'].isna() & \n",
    "                                                df['LotFrontage_quad'].isna()) \n",
    "\n",
    "# Notice how we are using single ('|' and '&'), NOT  double (|| and &&)  operators. \n",
    "# This is because the operations are component-wise (row-by-row) which is sort of analogous to 'bit-wise' \n",
    "\n",
    "df[ ['LotFrontage', 'LotFrontage_quad', 'LotFrontage_quad2', 'equality_test', 'equality_test_2']].head( 20 )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to tell whether all results in equality_test_2 are true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['equality_test_2'].all() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Yes, they are! Great SUCCESS!\n",
    "<img src=\"https://media.giphy.com/media/a0h7sAqON67nO/giphy.gif\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is also an `apply` method defined on the `DataFrame` class. That gives access to a whole row at a time. \n",
    "\n",
    "In other words, you define a function that takes a *whole row* as an argument, not just a value, and computes a result from it.\n",
    "\n",
    "The result of applying this function will be a series.\n",
    "\n",
    "Example: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def lot_irregularity(  row ) : \n",
    "    \"\"\"Compute an irregularity factor\"\"\"\n",
    "    if row[\"LotShape\"] == \"Reg\" : \n",
    "        return math.sqrt( row[\"LotArea\"] ) / row[\"LotFrontage\"] \n",
    "    else : \n",
    "        return row[\"LotArea\"] / ( row[\"LotFrontage\"] ** 2 ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['LotIrregularity'] = df.apply( lot_irregularity, axis = 1 ) \n",
    "# axis = 1 means to apply function on each row, axis = 0 would be to apply the function on each colum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['LotShape', 'LotArea', 'LotFrontage', 'LotIrregularity']].head(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise E5 ** \n",
    "\n",
    "Write a function `lot_irregularity_v2` that  returns 1.0 when `LotFrontage` is `NaN` and returns the value returned by `lot_irregularity`, otherwise.\n",
    "\n",
    "You can test this with `math.isnan( row[\"LotFrontage\"] )`\n",
    "\n",
    "Then apply this function to define a new column called `\"LotIrregularity_v2\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Your codez here ....\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ans_submit( \"E5\", round( df[\"LotIrregularity_v2\"].sum(), 3) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering\n",
    "\n",
    "Filtering refers to extracting a *subset of a dataset*.\n",
    "\n",
    "A 'subset' can refer to either a subset of the columns in which case more common terms are **selection**, and **projection** for the mathematical purists. \n",
    "\n",
    "Most commonly, 'filtering' refers to taking a subset of the rows according to some criterion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selection (or projection) \n",
    "\n",
    "We have already seen the basic method for selecting a few columns: `df[ list-of-columns ]`\n",
    "\n",
    "A slightly different method is   `df.loc[ :,  boolean-mask ]`\n",
    " \n",
    "Here, 'boolean mask' means an array of booleans of length equal to the number of columns in `df`\n",
    "\n",
    "For example, if we want to just select all columns with info related to the basement, we can do as follows. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basement_related_cols = df.columns.str.contains(\"Bsmt\")\n",
    "basement_related_cols # this is the mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.loc[ :, basement_related_cols].head( 15)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`loc` is an accessor that yield access to \n",
    "\n",
    "Everything about loc here \n",
    "https://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.loc.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise F1 ** \n",
    "\n",
    "Create a mask to indentify all columns that end on 'SF' (this means surface area). Hint: First run `help(df.columns.str)` and see what method could help make this task easier. \n",
    "\n",
    "Then define a new dataframe `df_sf = df.loc[: , your_mask_here]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here: define mask, define df_sf \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ans_submit( \"F1\", df_sf.sum().sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filtering  rows\n",
    "\n",
    "The method for filtering rows that is used 95% of the time is simple boolean filtering as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_row_mask = (df['LotArea']  >= 10000)  # this defines a boolean Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['bool_mask'] = boolean_row_mask\n",
    "df[['LotArea', 'bool_mask']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_row_mask.head( 20 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "type( boolean_row_mask )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boolean_row_mask.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this selects all house with a lot area strictly greater than or equal 10000\n",
    "df_biglot = df[ boolean_row_mask ] \n",
    "pd.set_option( \"display.max_columns\", 10 )\n",
    "df_biglot.head( 25 )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biglot.shape # This yields the number of rows and columns as pair ( 2-tuple )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Naturally, it is possible and even preferable in most cases to simply use the expression that defines the mask\n",
    "inside the brackets, without having to store it a name. \n",
    "\n",
    "This has the following advantages: (1) memory preservation,  (2) avoids name pollution and (3) makes the code more concise\n",
    "\n",
    "The only disadvantage is that it makes code (slightly) less readable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_biglot = df[ df['LotArea']  >= 10000 ] \n",
    "df_biglot.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise F2 ** \n",
    "\n",
    "Define a subset of `df` called `df_ss` containing all records which satisfy *all* of the following conditions\n",
    " \n",
    "  * `HeatingQC` equals `'Ex'` \n",
    "  * `LotFrontage`  greater than or equal to `50.0`\n",
    "  * `LotShape` different from `'Reg'`\n",
    "  \n",
    " \n",
    "**Important Warning:** when combining comparisons with Boolean operators you should always put the comparisons in parenthesis. Unfortunately, the operator precedence is not what you are used to from other languages \n",
    " \n",
    "Check your result visually (displaying those columns) before submitting..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Your code here \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( 'F2', df_ss.shape[0] )   # df_ss.shape[0] is how you tell how many rows df_ss has."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### head / tail methods\n",
    "\n",
    "The other basic method for taking a subset of rows from a dataset is to make use of the fact that (unlike SQL tables in most implementations) dataframes have an intrisic order. Thus one can talk about the first `n` rows or the last `n` rows. \n",
    "\n",
    "There are two corresponding methods: \n",
    "\n",
    "  * `df.head( n )` returns a new dataframe consisting of the **first** `n` rows of df \n",
    "  * `df.tail( n )` returns a new dataframe consisting of the **last** `n` rows of df\n",
    "  \n",
    "You have seen many examples of using `head()` above..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise F3 **\n",
    "\n",
    "Define `df_t20` to be the last 20 rows of `df`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( 'F3', df_t20['LotArea'].sum() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summarizing  / Descriptive statistics\n",
    "\n",
    "The Series class has several predefined methods to quickly calculate summaries of the data in it.\n",
    "\n",
    "The following methods work on Series of any datatype:\n",
    "\n",
    "  * `ser.count()` : count of non-NA values \n",
    "  * `ser.nunique()` : count of unique values \n",
    "  * `ser.value_counts()` : yields a table (actually just a series) with the distinct values and the count for each. This is know by statatisticians as the *frequency table* \n",
    "  * `ser.mode()` : The most frequent unique element in the series \n",
    "\n",
    "  * `ser.describe()` : computes several important statistics. It's output depends on whether the series is numeric or not. See exercise below and the documentation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise S1 ** \n",
    "\n",
    "Try applying `.describe()` to any non-numeric column in `df`. You will notice that one of the outpus is called `top`. The corresponding value is a value from the series. Which of the methods listed above could be used to compute this value by it self?  Go to the documentation of `pandas.Series.describe` to find out...  When submitting the answer just give the name of the method as a single word, with no `ser.` at the beginning and no `()` at the end."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( \"S1\", \"name of method here as a single word with no punctuation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For numeric columns we have a few more methods: \n",
    "        \n",
    " * `ser.sum()` : which \n",
    " * `ser.mean()` : which computes the mean \n",
    " * `ser.max()` and `ser.min()`: compute the maximum and minimum value of the series \n",
    " * `ser.median()` : The 50th-percentile, i.e. the value M such that 50% of the values in the series are less than or equal to M and 50% of the values are greater than or equal to M. \n",
    " * `ser.quantile(x)` : Returns the x-quantile of the distribution of x, i.e the value Q such that (x*100)% of the values of the series are less than or equal to Q."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise S2**\n",
    "\n",
    "Try applying `.describe()` to a numeric column in `df`, you will see several outputs, such as `count`, `mean`, `min`, etc...\n",
    "One of the listed outputs cannot be computed by any of the functions listed above. Which one? \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit(\"S2\", ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise S3 **\n",
    "\n",
    "All methods on a numeric Series skip NaN values by default. However, they all admit an optional argument to control this behaviour. \n",
    "\n",
    "Go to the documentation of any of these methods to figure out what that argument is called"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit(\"S3\", \"name of optional argument to control whether to skip NaN values when computing a summary\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise S4 **\n",
    "\n",
    "Answer \"True\" or \"False\", when sumarizing a numeric series, skipping NaN values is equivalent to treating them as if they where `0.0`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( \"S4\" , \"True\" / \"False\" )  # leave just one of the two "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Special methods for Boolean series\n",
    "\n",
    "Booleans Series are specially simple and also specially useful. They usually give the answer to a yes/no question about the each cell in another series or each row in a DataFrame.\n",
    "\n",
    "We have seen how boolean series are produce above, by computing expression of the form: `another_series == some_value`, `another_series > some_threshold` or simply `another_series.isna()`\n",
    "\n",
    "There are two very useful method for Boolean series, namely: \n",
    "\n",
    "  * `ser.all()`: Returns True if and only if all elements of the boolean series are 'True' \n",
    "  * `ser.any()`: Returns True if any of the elements of the series are True\n",
    "  \n",
    "**ACHTUNG:** boolean series cannot have any NaN values. True and False are the only allowed values. In fact, if a series from another type is coerced to boolean type, for instance calling `ser.astype( bool )`, then `NaN` values turn into True!  \n",
    "This is significantly different from SQL semantics where NULL efectively acts as False for filtering operations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Exercise S5 **\n",
    "\n",
    "What is the result of applying `.sum()` to a boolean Series called `ser` and having `n` elements? \n",
    "\n",
    " * **a.** Since `+` is equivalent to `or` for bools the result is:  `ser[0] or ser[1] or ser[2] or ... or ser[n-1]`, \n",
    "    i.e. the same as applying `.all()`  \n",
    " * **b.** Bool is the same type as the integers modulo 2, thus  `+` is equivalent  to `xor` (1 + 1 = 0 modulo 2) and the result is `ser[0] xor ser[1] xor ... xor ser[n-1]?  \n",
    " * **c.** Pragmatism beats purity and the result is the same as first casting the whole series to type `int` (False -> 0, True -> 1) and summing the resulting elements as regular integers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans_submit( \"S5\", \"a\" / \"b\" or \"c\") # leave one"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To discover all predefined predefined methods for computing descriptive statistics go here. \n",
    "https://pandas.pydata.org/pandas-docs/stable/api.html#computations-descriptive-stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "  * The pandas home: https://pandas.pydata.org/\n",
    "  * The pandas API:  \n",
    "      * Dataframe: https://pandas.pydata.org/pandas-docs/stable/api.html#dataframe\n",
    "      * Series: https://pandas.pydata.org/pandas-docs/stable/api.html#series\n",
    "      * All about indexing and selection https://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-label\n",
    "  \n",
    "  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
