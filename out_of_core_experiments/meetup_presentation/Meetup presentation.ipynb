{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Big data processing on a single machine with Python!\n",
    "\n",
    "<div>\n",
    "    <img src=\"portada.jpg\" alt=\"many workers handling big data on a single computer\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import pandas  as pd\n",
    "DATA_DIR=\"C:/_DATA/experimentation/\"\n",
    "checkins_df = pd.read_csv( DATA_DIR + 'Gowalla_totalCheckins.txt', delimiter='\\t', header=None )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "checkins_df.columns = [\"user_id\", \"checkin_ts\", \"latitude\", \"longitud\", \"location_id\"]\n",
    "checkins_df1 = checkins_df[ checkins_df.location_id % 1000 == 1] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## A (big data?) problem:\n",
    "\n",
    "We are given data from a social networking site:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table border=\"1\" class=\"dataframe\">\n",
    "  <thead>\n",
    "    <tr style=\"text-align: right;\">\n",
    "      <th></th>\n",
    "      <th>user_id</th>\n",
    "      <th>checkin_ts</th>\n",
    "      <th>latitude</th>\n",
    "      <th>longitud</th>\n",
    "      <th>location_id</th>\n",
    "    </tr>\n",
    "  </thead>\n",
    "  <tbody>\n",
    "    <tr>\n",
    "      <th>68389</th>\n",
    "      <td>241</td>\n",
    "      <td>2010-10-14T02:09:59Z</td>\n",
    "      <td>47.611588</td>\n",
    "      <td>-122.200418</td>\n",
    "      <td>2158038</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>73200</th>\n",
    "      <td>247</td>\n",
    "      <td>2010-09-29T18:34:05Z</td>\n",
    "      <td>49.246997</td>\n",
    "      <td>-123.002488</td>\n",
    "      <td>21782</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>13788</th>\n",
    "      <td>71</td>\n",
    "      <td>2010-03-20T20:20:47Z</td>\n",
    "      <td>39.293855</td>\n",
    "      <td>-94.720260</td>\n",
    "      <td>547748</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>63127</th>\n",
    "      <td>222</td>\n",
    "      <td>2009-12-17T18:33:13Z</td>\n",
    "      <td>42.074845</td>\n",
    "      <td>-87.740850</td>\n",
    "      <td>152179</td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "      <th>73875</th>\n",
    "      <td>247</td>\n",
    "      <td>2010-07-08T23:59:37Z</td>\n",
    "      <td>49.277759</td>\n",
    "      <td>-122.858863</td>\n",
    "      <td>13217</td>\n",
    "    </tr>\n",
    "  </tbody>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "# print( df.iloc[ list(np.random.choice( 100000, 5 )), :].to_html()) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Each row corresponds to a check-in event: user checks-in at a location at a given time.\n",
    "\n",
    "\n",
    "The input dataset consists of 6.4 MM records "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Problem: detect stalker-stalkee pairs\n",
    "\n",
    "\n",
    "If $E$ and $R$ are two users, we define the **stalking measure** between $E$ and $R$\n",
    "as the number of distinct locations $L$ such that $E$ visited $L$ and $R$ _also_ visited $L$ at a _later_ time.\n",
    "\n",
    "We want to find the top 5 pairs with the highest stalking measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Solution: with Pandas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "checkins_by_loc = (checkins_df1[['user_id', 'checkin_ts', 'location_id']] \n",
    "                      .set_index('location_id') ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chin_pairs = checkins_by_loc.join( checkins_by_loc, lsuffix='_ee', rsuffix='_er' ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pairs_filtered = (chin_pairs[(chin_pairs.checkin_ts_ee < chin_pairs.checkin_ts_er) &                                                 \n",
    "                             (chin_pairs.user_id_ee != chin_pairs.user_id_er )]\n",
    "                      .rename( columns= {\"user_id_er\" : \"stalker\",\n",
    "                                         \"user_id_ee\" : \"stalkee\" }) \n",
    "                      .reset_index()\n",
    "                      [[\"stalkee\", \"stalker\", \"location_id\"]] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "final_result = ( pairs_filtered.drop_duplicates()                 \n",
    "                     .groupby([\"stalkee\", \"stalker\"])\n",
    "                     .agg( {\"location_id\" : \"count\"})\n",
    "                     .rename( columns = { \"location_id\" : \"location_count\" } )                 \n",
    "                     .sort_values('location_count', ascending=False) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    " * <span style=\"color:red\"> MEMORY ERROR!</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Why the out-of-memory error? \n",
    "\n",
    "\n",
    "* If there are $n_l$ check-ins at location $l$, then `chin_pairs` contains $n_l^2$ records for this location. \n",
    "\n",
    "   * So, a location with $n_l = 1000$ checkins generates $1,000,000$ records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In total, we  would get $\\sum_l$ $n_l^2 $ records in total."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>location_id</th>\n",
       "      <th>8904</th>\n",
       "      <th>8932</th>\n",
       "      <th>8936</th>\n",
       "      <th>8938</th>\n",
       "      <th>8947</th>\n",
       "      <th>8954</th>\n",
       "      <th>8956</th>\n",
       "      <th>8957</th>\n",
       "      <th>8958</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>n_l</th>\n",
       "      <td>12</td>\n",
       "      <td>16</td>\n",
       "      <td>12</td>\n",
       "      <td>130</td>\n",
       "      <td>570</td>\n",
       "      <td>18</td>\n",
       "      <td>61</td>\n",
       "      <td>36</td>\n",
       "      <td>42</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "location_id  8904  8932  8936  8938  8947  8954  8956  8957  8958\n",
       "n_l            12    16    12   130   570    18    61    36    42"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chins_by_loc = ( checkins_df.groupby( \"location_id\" )\n",
    "                                .agg( {\"location_id\" : \"count\"} )\n",
    "                                .rename( columns = {\"location_id\" : \"n_l\"} ) ) \n",
    "\n",
    "num_chins_by_loc.head(9).transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "561828204"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_chin_pairs = int( (num_chins_by_loc ** 2).sum() )\n",
    "num_chin_pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second attempt \n",
    "    \n",
    "*  Sort the data by location (in memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  For each $L$ location produce all tuples $(E,R,L)$ such that $E$ visited $L$ a $R$ visited $L$ at a later time. Store this in disk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  Sort the resulting files so that we have all tuples with the same $(E,R)$ appear in the same file. Delete previous set of files and store newly sorted files in disc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "*  For each $(E,R)$ combination count the number of distinct $L$s to compute the stalking measure. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The procedure above looks a lot like **map-reduce**, but on a single computer...\n",
    "\n",
    " * Took 3 hours to code in Python\n",
    " * and **5 hours** to run to completion \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "  * Is it possible to process _big data_ on a laptop?\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * Is it convenient to process _big data_ on a laptop?\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  *  **Big data** (one definition):  \n",
    "    * Data is big whenever doesn't fit in RAM of a _single_ computer.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "  * Big data computation is _not necessarily_ distributed\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The concept of out-of-core computation\n",
    "\n",
    "When your data doesn't fit in RAM but it does fit on your disk:\n",
    "  \n",
    "  * Load one chunk of input data, do something with it, write intermediate result to disk (if necessary)\n",
    "  \n",
    "  \n",
    "  * Drop input data chunk (from memory)\n",
    "  \n",
    "  \n",
    "  * Load the next chunk of data, do something with it, write intermediate result to disk (if necessary)\n",
    "  \n",
    "  \n",
    "  * Rinse, repeat...  \n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But wait... \n",
    "\n",
    "A disgression into _access_ latencies\n",
    "\n",
    "| System Event |\tActual Latency\t|\n",
    "|--------------|-------------------:|\n",
    "| One CPU cycle |\t0.4 ns\t| \n",
    "| Level 1 cache access |\t0.9 ns\t|\n",
    "| Level 2 cache access |\t2.8 ns\t|\n",
    "| Level 3 cache access | 28 ns | \n",
    "| Main memory access (DDR DIMM) | ~100 ns |\n",
    "| SSD I/O | 50–150 μs | \n",
    "| Magnetic disk I/O | 1–10 ms | \n",
    "| Internet call: San Francisco to New York City | 65 ms |\n",
    "| Internet call: San Francisco to Hong Kong | 141 ms |\n",
    "\n",
    "** * Latency**: time you have to wait for the _first byte_ to arrive \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But wait... \n",
    "\n",
    "Scaling to human scale:  1 CPU cycle $\\to$ 1 s ($\\approx 1$ heartbeat) \n",
    "\n",
    "| System Event                |\tActual Latency\t| Scaled Latency | Human event |\n",
    "|-----------------------------|-----------------:|----------------|-----------|\n",
    "| One CPU cycle |\t0.4 ns\t        | 1 s | 1 heartbeat | \n",
    "| Level 1 cache access |\t0.9 ns\t| 2 s | remembering something |\n",
    "| Level 2 cache access |\t2.8 ns\t| 7 s | checking notebook | \n",
    "| Level 3 cache access | 28 ns      | 1 min | looking at a book  |\n",
    "| Main memory access (DDR DIMM) | ~100 ns | 4 min | calling a friend  |\n",
    "| SSD I/O | 50–150 μs                                   | 1.5–4 days | getting a reply by letter | \n",
    "| Rotational disk I/O | 1–10 ms                         | 1–9 months | making a baby |\n",
    "| Internet call: San Francisco to New York City | 65 ms | 5 years | finishing an undergrad |\n",
    "| Internet call: San Francisco to Hong Kong | 141 ms    | 11 years | doing undergrad + PhD | \n",
    " \n",
    " \n",
    "** * Latency**: time you have to wait for the _first byte_ to arrive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## But wait...\n",
    "\n",
    "How about _throughput_?\n",
    "\n",
    "| Reading from source: |  Time taken to read 1000 MB  |\n",
    "|----------------------| -----------------------------|\n",
    "| Main memory DDR3 or DDR4 <br /> (L1/2 caches are faster) |  40 ms |\n",
    "| Solid state drive (SSD - SATA)  |  750 ms |\n",
    "| Magnetic hard drive (HDD)   | 10 s | \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Conclusion: \n",
    "\n",
    "\n",
    "Not all discs were created equal!\n",
    "  \n",
    "  * Reading big amounts of data from a magnetic HDD is about 200x slower than from memory.\n",
    "\n",
    "\n",
    "  * Doing the same from SDD is only about 20x as slow... could be acceptable!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Second problem with out of core computation...\n",
    "\n",
    "* A simple data analyst should **not** have to worry about splitting data in chunks, storing intermediate results in disc, releasing memory, reloading ...\n",
    "\n",
    "\n",
    "\n",
    "* This is **error prone**,  **tedious** and **distracts** us from the main goal, to design a nice data-analysis algorithm \n",
    "\n",
    "\n",
    "\n",
    "* What we want is a software framework that does this for us and abstracts all the complexity!\n",
    "\n",
    "\n",
    "* Something like **Turicreate**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Turicreate to the rescue!\n",
    "\n",
    "### What is Turicreate?\n",
    "\n",
    "   * Formerly graphlab-create and closed-source.\n",
    "\n",
    "   \n",
    "   * Acquired by Apple Inc. and open-sourced   \n",
    "   \n",
    "   \n",
    "   * Available at: https://github.com/apple/turicreate\n",
    "   \n",
    "   \n",
    "   * Facilities for manipulating both structured and unstructured data \n",
    "   \n",
    "   \n",
    "   * Develop ML / DL models\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Why do we care?\n",
    "\n",
    "   * Based on **SFrames** library, very much like Pandas DataFrames but *out-of-core*!\n",
    "   \n",
    "   \n",
    "   * Very similar API $\\to$ easy translation!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [],
   "source": [
    "checkins = checkins_sf[[\"user_id\", \"location_id\", \"checkin_ts\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "chin_ps = ( checkins.join( checkins, on = 'location_id' )\n",
    "                    .rename( {'checkin_ts' : 'checkin_ts_ee',\n",
    "                              'checkin_ts.1' : 'checkin_ts_er',\n",
    "                              'user_id' : 'stalkee' ,\n",
    "                              'user_id.1' : 'stalker' }) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "pairs_filtered = chin_ps[ (chin_ps['checkin_ts_ee'] < chin_ps['checkin_ts_er']) &\n",
    "                          (chin_ps['stalkee'] != chin_ps['stalker']) ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "final_result = ( pairs_filtered[[ 'stalkee', 'stalker', 'location_id']]\n",
    "                     .unique()\n",
    "                     .groupby( ['stalkee','stalker'] ,\n",
    "                               {\"location_count\" : agg.COUNT })\n",
    "                     .topk( 'location_count', k=5, reverse=False )\n",
    "                     .materialize() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### It works!\n",
    "\n",
    "        Inferred types from first 100 line(s) of file as\n",
    "        column_type_hints=[int,str,float,float,int]\n",
    "        ------------------------------------------------------\n",
    "        Read 870755 lines. Lines per second: 520097\n",
    "        Finished parsing file /home/ubuntu/turicreate_experiment/Gowalla_totalCheckins.txt\n",
    "        Parsing completed. Parsed 6442892 lines in 5.43026 secs.\n",
    "\n",
    "        +---------+-----------+----------------+\n",
    "        | stalkee |  stalker  | location_count |\n",
    "        +---------+-----------+----------------+\n",
    "        |   1251  |   106819  |      388       |\n",
    "        |  10410  |   10393   |      365       |\n",
    "        |  40090  |   132961  |      361       |\n",
    "        |   1404  |    1080   |      330       |\n",
    "        |  18446  |   106815  |      326       |\n",
    "        +---------+-----------+----------------+\n",
    "        [5 rows x 3 columns]\n",
    "        \n",
    "* Took ~1700 secs on an AWS Ubuntu machine with **4 GB** of RAM  and **16GB SSD.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Other alternatives (in the Python ecosystem)\n",
    "\n",
    "\n",
    "### **Ray**: https://github.com/ray-project/ray\n",
    "\n",
    "<div>\n",
    "    <img src=\"ray.png\" />\n",
    " </div>\n",
    "\n",
    "  \n",
    "  * \"Ray is a flexible, high-performance distributed execution framework.\"\n",
    "  \n",
    "  * `pip install ray` but doesn't work for Windows :( \n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### **Dask:** https://dask.pydata.org/en/latest/\n",
    "\n",
    "<br />\n",
    "<div>\n",
    "    <img src=\"dask.png\" />\n",
    " </div>\n",
    " \n",
    "* \"Dask provides advanced parallelism for analytics, enabling performance at scale for the tools you love\"\n",
    "  * Runs on Windows!\n",
    "  * Built on top of the standard PyData stack: `numpy` / `pandas` / `scikit-learn`\n",
    "  * still not mature\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Summary\n",
    "\n",
    "\n",
    "* It is possible an even convenient to process (modereately) Big Data on a laptop!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* When handling big data access speeds and throughput measurements matter!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* Not all disks are created equal!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "* There are nice tools por out-of-core Big Data computation in the Python ecosystem!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## References \n",
    "\n",
    "Code show is to be found here: https://github.com/YuxiGlobal/data-analytics/tree/master/out_of_core_experiments\n",
    "\n",
    "https://software.intel.com/en-us/articles/memory-performance-in-a-nutshell\n",
    "    \n",
    "http://www.prowesscorp.com/computer-latency-at-a-human-scale/\n",
    "    \n",
    "https://medium.com/@mateini_12893/python-for-big-data-computation-on-a-single-computer-c232046df3c3\n",
    "    \n",
    "https://www.analyticsvidhya.com/blog/2018/03/pandas-on-ray-python-library-make-processing-faster/\n",
    "\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ganadores de premios\n",
    "\n",
    "1. @Aveldarrama\n",
    "2. Ricardo Ruíz Cortés\n",
    "3. Yovany Alvarez Correa \n",
    "4. Luis Eduardo Lopez\n",
    "5. Cristian Orozco\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
